{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "POOUQwkRuyW5",
   "metadata": {
    "id": "POOUQwkRuyW5"
   },
   "source": [
    "https://www.kaggle.com/code/vucongtuanduong/time-hardmargin-svm-tfidf/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8644d174",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T00:54:32.545229Z",
     "iopub.status.busy": "2025-04-12T00:54:32.544920Z",
     "iopub.status.idle": "2025-04-12T00:54:37.612643Z",
     "shell.execute_reply": "2025-04-12T00:54:37.611700Z"
    },
    "id": "8644d174",
    "outputId": "37da899a-9026-489e-aa1e-6aef524bfcf0",
    "papermill": {
     "duration": 5.073505,
     "end_time": "2025-04-12T00:54:37.614375",
     "exception": false,
     "start_time": "2025-04-12T00:54:32.540870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.stem import SnowballStemmer\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "#Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Feature Engineering\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#Evaluation Metric\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,f1_score, precision_score,recall_score,classification_report\n",
    "import seaborn as sns\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d937a6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T00:54:37.622393Z",
     "iopub.status.busy": "2025-04-12T00:54:37.621424Z",
     "iopub.status.idle": "2025-04-12T00:54:37.649879Z",
     "shell.execute_reply": "2025-04-12T00:54:37.648904Z"
    },
    "id": "9d937a6d",
    "papermill": {
     "duration": 0.033854,
     "end_time": "2025-04-12T00:54:37.651474",
     "exception": false,
     "start_time": "2025-04-12T00:54:37.617620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HardMarginSVM:\n",
    "    \"\"\"\n",
    "    Optimized Hard Margin SVM implementation using gradient descent\n",
    "\n",
    "    Attributes\n",
    "    -------------\n",
    "    eta : float\n",
    "        Learning rate\n",
    "    epoch : int\n",
    "        Number of epochs\n",
    "    random_state : int\n",
    "        Random seed\n",
    "    is_trained : bool\n",
    "        Training completion flag\n",
    "    num_samples : int\n",
    "        Number of training samples\n",
    "    num_features : int\n",
    "        Number of features\n",
    "    w : NDArray[float]\n",
    "        Parameter vector: (num_features, ) ndarray\n",
    "    b : float\n",
    "        Bias parameter\n",
    "    alpha : NDArray[float]\n",
    "        Lagrange multipliers: (num_samples, ) ndarray\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.001, epoch=1000, random_state=42):\n",
    "        self.eta = eta\n",
    "        self.epoch = epoch\n",
    "        self.random_state = random_state\n",
    "        self.is_trained = False\n",
    "        self.support_vectors = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit parameter vector to training data\n",
    "\n",
    "        Parameters\n",
    "        --------------\n",
    "        X : NDArray[NDArray[float]]\n",
    "            Training data: (num_samples, num_features) matrix\n",
    "        y : NDArray[float]\n",
    "            Training labels: (num_samples) ndarray\n",
    "        \"\"\"\n",
    "        # Convert sparse matrix to dense if needed\n",
    "        if hasattr(X, \"toarray\"):\n",
    "            X = X.toarray()\n",
    "\n",
    "        self.num_samples = X.shape[0]\n",
    "        self.num_features = X.shape[1]\n",
    "\n",
    "        y_unique = np.unique(y)\n",
    "        if len(y_unique) != 2:\n",
    "            raise ValueError(\"Binary classification requires exactly 2 classes\")\n",
    "\n",
    "        if set(y_unique) == {0, 1}:\n",
    "            y = np.where(y == 0, -1, 1)\n",
    "\n",
    "        self.w = np.zeros(self.num_features)\n",
    "        self.b = 0\n",
    "\n",
    "\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.alpha = rgen.uniform(low=0.0, high=0.01, size=self.num_samples)\n",
    "        for i in range(self.epoch):\n",
    "            self._cycle(X, y)\n",
    "\n",
    "        sv_indices = np.where(self.alpha != 0)[0]\n",
    "\n",
    "        self.support_vectors = sv_indices\n",
    "\n",
    "        self.w = np.zeros(self.num_features)\n",
    "        for i in sv_indices:\n",
    "            self.w += self.alpha[i] * y[i] * X[i]\n",
    "\n",
    "        bias_sum = 0\n",
    "        for i in sv_indices:\n",
    "            bias_sum += y[i] - np.dot(self.w, X[i])\n",
    "\n",
    "        self.b = bias_sum / len(sv_indices)\n",
    "\n",
    "        self.is_trained = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Return predictions\n",
    "\n",
    "        Parameters\n",
    "        --------------\n",
    "        X : NDArray[NDArray[float]]\n",
    "            Data to classify: (any, num_features) matrix\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        result : NDArray[int]\n",
    "            Classification results 0 or 1: (any, ) ndarray\n",
    "        \"\"\"\n",
    "        if not self.is_trained:\n",
    "            raise Exception('Model not trained yet')\n",
    "\n",
    "        # Convert sparse matrix to dense if needed\n",
    "        if hasattr(X, \"toarray\"):\n",
    "            X = X.toarray()\n",
    "\n",
    "        decision_values = X @ self.w + self.b\n",
    "\n",
    "        result = np.where(decision_values >= 0, 1, 0)\n",
    "        return result\n",
    "\n",
    "    def _cycle(self, X, y):\n",
    "        \"\"\"\n",
    "        One gradient descent cycle\n",
    "\n",
    "        Parameters\n",
    "        --------------\n",
    "        X : NDArray[NDArray[float]]\n",
    "            Training data: (num_samples, num_features) matrix\n",
    "        y : NDArray[float]\n",
    "            Training labels: (num_samples) ndarray\n",
    "        \"\"\"\n",
    "        y = y.reshape([-1, 1])\n",
    "\n",
    "        XXT = X @ X.T\n",
    "        H = (y @ y.T) * XXT\n",
    "\n",
    "        grad = np.ones(self.num_samples) - H @ self.alpha\n",
    "\n",
    "        self.alpha += self.eta * grad\n",
    "\n",
    "        self.alpha = np.clip(self.alpha, 0, None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369fcf41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T00:54:37.657884Z",
     "iopub.status.busy": "2025-04-12T00:54:37.657322Z",
     "iopub.status.idle": "2025-04-12T00:54:37.663214Z",
     "shell.execute_reply": "2025-04-12T00:54:37.661939Z"
    },
    "id": "369fcf41",
    "outputId": "25dc63b6-cbd1-4a77-ee7e-5e617dc83c65",
    "papermill": {
     "duration": 0.010876,
     "end_time": "2025-04-12T00:54:37.664945",
     "exception": false,
     "start_time": "2025-04-12T00:54:37.654069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000, 5500, 6000, 6500, 7000, 7500, 8000, 8500, 9000, 9500]\n"
     ]
    }
   ],
   "source": [
    "a = [i for i in range(1000, 10000, 500)]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9bbb748",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T00:54:37.670806Z",
     "iopub.status.busy": "2025-04-12T00:54:37.670435Z",
     "iopub.status.idle": "2025-04-12T00:54:37.683875Z",
     "shell.execute_reply": "2025-04-12T00:54:37.683020Z"
    },
    "id": "f9bbb748",
    "papermill": {
     "duration": 0.018031,
     "end_time": "2025-04-12T00:54:37.685295",
     "exception": false,
     "start_time": "2025-04-12T00:54:37.667264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_time_svm():\n",
    "  results = []\n",
    "  for i in a:\n",
    "    df = pd.read_csv(f\"https://media.githubusercontent.com/media/PTIT-Assignment-Projects/ai-svm-email-spam/refs/heads/main/dataset/sampled_dataset{i}.csv\")\n",
    "    def remove_special_characters(word):\n",
    "        return re.sub(r'[^a-zA-Z\\s]', '', word)\n",
    "    ENGLISH_STOP_WORDS = set(stopwords.words('english'))\n",
    "    def remove_stop_words(words):\n",
    "        return [word for word in words if word not in ENGLISH_STOP_WORDS]\n",
    "    def remove_url(word):\n",
    "        return re.sub(r\"http\\S+\", \"\", word)\n",
    "    df['text'] = df['text'].apply(remove_special_characters)\n",
    "    df['text'] = df['text'].apply(remove_url)\n",
    "    df['text'] = df['text'].apply(word_tokenize)\n",
    "    df['text'] = df['text'].apply(remove_stop_words)\n",
    "    df['text'] = df['text'].apply(' '.join)\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    def stem_text(text):\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        return ' '.join(stemmed_tokens)\n",
    "    df['text'] = df['text'].apply(stem_text)\n",
    "    # dataset trainning voi test\n",
    "    X = df['text']\n",
    "    y = df['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "\n",
    "    # tfidf\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "    # # hashing_vectorizer\n",
    "    # hashing_vectorizer = HashingVectorizer(n_features=5000)\n",
    "    # X_train_hashed = hashing_vectorizer.fit_transform(X_train)\n",
    "    # X_test_hashed = hashing_vectorizer.transform(X_test)\n",
    "    X_train_dense = X_train_tfidf.toarray()\n",
    "    X_test_dense = X_test_tfidf.toarray()\n",
    "    result = []\n",
    "    svm_base = HardMarginSVM()\n",
    "    start_time = time.time()\n",
    "    svm_base.fit(X_train_dense, y_train)\n",
    "    end_time = time.time()\n",
    "    y_pred = svm_base.predict(X_test_dense)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    result.append({\n",
    "            'class_name': svm_base.__class__.__name__,\n",
    "            'time': end_time - start_time,\n",
    "            'accuracy_score': accuracy,\n",
    "            'f1_score': f1\n",
    "        })\n",
    "    results_df = pd.DataFrame(result)\n",
    "    print(results_df)\n",
    "    results_df.to_csv(f'hardmargin_svm_tfidf_{i}.csv', index=False)\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d288ee9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-12T00:54:37.691172Z",
     "iopub.status.busy": "2025-04-12T00:54:37.690799Z"
    },
    "id": "d288ee9f",
    "outputId": "44b7330b-8b4d-4bbd-d6c9-e3082618ad21",
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2025-04-12T00:54:37.687623",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name        time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  188.271099           0.945   0.94686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name        time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  483.728427            0.96  0.963415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name         time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  1025.564333            0.96  0.961353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name         time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  1769.558942           0.976  0.976378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name         time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  2834.332839        0.966667  0.966887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name         time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  4327.781785        0.967143  0.969935\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name         time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  6007.968148         0.96375  0.967306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name         time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  8247.298119        0.962222  0.963907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      class_name          time  accuracy_score  f1_score\n",
      "0  HardMarginSVM  10635.539112           0.969  0.970837\n"
     ]
    }
   ],
   "source": [
    "calc_time_svm()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-12T00:54:27.572405",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
