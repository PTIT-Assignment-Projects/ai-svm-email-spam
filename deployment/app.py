import streamlit as st
import pandas as pd
import nltk
import numpy as np
nltk.download('stopwords')
nltk.download('punkt_tab')
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import re
from nltk.stem import SnowballStemmer
import time
import pickle
import string
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import csr_matrix

@st.cache_resource
def download_nltk_resources():
    nltk.download('stopwords')
    nltk.download('punkt_tab')
    
download_nltk_resources()

class SVM:
    def __init__(self, lambda_param=1e-4, epoch=1000, batch_size=256, tol=1e-4, random_state=42):
        self.lambda_param = lambda_param
        self.epoch = epoch
        self.batch_size = batch_size
        self.tol = tol
        self.random_state = random_state
        self.is_trained = False

    def fit(self, X, y):
        if hasattr(X, "toarray"):
            X = csr_matrix(X)
        
        self.num_samples, self.num_features = X.shape

        y_unique = np.unique(y)
        if len(y_unique) != 2:
            raise ValueError("Ph√¢n lo·∫°i nh·ªã ph√¢n c·∫ßn 2 nh√£n")
        if set(y_unique) == {0, 1}:
            y = np.where(y == 0, -1, 1)
        
        self.w = np.zeros(self.num_features, dtype=np.float32)
        self.b = 0.0

        np.random.seed(self.random_state)
        t = 0
        previous_objective = float("inf")

        for ep in range(1, self.epoch + 1):
            indices = np.random.permutation(self.num_samples)
            for start in range(0, self.num_samples, self.batch_size):
                t += 1
                end = start + self.batch_size
                batch_idx = indices[start:end]
                X_batch = X[batch_idx]
                y_batch = y[batch_idx]
                
                eta = 1.0 / (self.lambda_param * t)
                margins = y_batch * (X_batch.dot(self.w) + self.b)
                mask = margins < 1
                self.w *= (1 - eta * self.lambda_param)
                if np.any(mask):
                    X_violate = X_batch[mask]
                    y_violate = y_batch[mask]
                    self.w += (eta / self.batch_size) * np.dot(y_violate, X_violate.toarray() if hasattr(X_violate, "toarray") else X_violate)
                    self.b += (eta / self.batch_size) * np.sum(y_violate)
                norm_w = np.linalg.norm(self.w)
                factor = min(1, (1.0 / np.sqrt(self.lambda_param)) / (norm_w))
                self.w *= factor

            decision = X.dot(self.w) + self.b
            hinge_losses = np.maximum(0, 1 - y * decision)
            objective = 0.5 * self.lambda_param * np.dot(self.w, self.w) + np.mean(hinge_losses)
            
            if ep % 10 == 0:
                print(f"Epoch {ep}, Gi√° tr·ªã h√†m m·ª•c ti√™u: {objective:.4f}")
            
            if abs(previous_objective - objective) < self.tol:
                print(f"D·ª´ng s·ªõm t·∫°i epoch {ep}, gi√° tr·ªã h√†m m·ª•c ti√™u thay ƒë·ªïi: {abs(previous_objective - objective):.6f}")
                break
            previous_objective = objective

        self.is_trained = True
        return self

    def predict(self, X):
        if not self.is_trained:
            raise Exception("M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c hu·∫•n lu·ªµen")
            
        if hasattr(X, "toarray"):
            X = csr_matrix(X)
            
        decision = X.dot(self.w) + self.b
        return np.where(decision >= 0, 1, 0)

@st.cache_resource
def load_model():
    try:
        with open('linear_svm.pkl', 'rb') as model_file:
            model = pickle.load(model_file)
        with open('vectorizer.pkl', 'rb') as vectorizer_file:
            vectorizer = pickle.load(vectorizer_file)
        return model, vectorizer
    except FileNotFoundError:
        st.error("Kh√¥ng t√¨m th·∫•y file model/vectorizer")
        return None, None

model, vectorizer = load_model()
ENGLISH_STOP_WORDS = set(stopwords.words('english'))
stemmer = SnowballStemmer('english')
def remove_special_characters(word):
    return re.sub(r'[^a-zA-Z\s]', '', word)
def remove_stop_words(words):
    return [word for word in words if word not in ENGLISH_STOP_WORDS]
def remove_url(word):
    return re.sub(r"http\S+", "", word)
def stem_text(text):
    tokens = nltk.word_tokenize(text)
    stemmed_tokens = [stemmer.stem(token) for token in tokens]
    return ' '.join(stemmed_tokens)
def preprocess_text(text):
    text = remove_special_characters(text)
    text = remove_url(text)
    text = word_tokenize(text)
    text = remove_stop_words(text)
    text = ' '.join(text)
    text = stem_text(text)
    return text
st.title("Demo ph√¢n lo·∫°i email ti·∫øng Anh spam ")
user_input = st.text_area("Nh·∫≠p n·ªôi dung email:", height=200)
if st.button("Ki·ªÉm tra"):
    if not user_input:
        st.warning("H√£y nh·∫≠p n·ªôi dung ƒë·ªÉ ph√¢n t√≠ch")
    elif model is None or vectorizer is None:
        st.error("Model/vectorizer kh√¥ng load ƒë∆∞·ª£c")
    else:
        with st.spinner("ƒêang ph√¢n t√≠ch..."):
            preprocessed_text = preprocess_text(user_input)
            if hasattr(vectorizer, 'transform'):
                features = vectorizer.transform([preprocessed_text])
            else:
                st.error("Kh√¥ng t√¨m th·∫•y vectorizer")
            prediction = model.predict(features)
            if prediction[0] == 1:
                st.error("üö® Email c√≥ kh·∫£ nƒÉng l√† SPAM")
            else:
                st.success("‚úÖEmail c√≥ kh·∫£ nƒÉng kh√¥ng ph·∫£i l√† SPAM ")
            
            st.write("### VƒÉn b·∫£n sau khi ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω:")
            st.write(preprocessed_text)